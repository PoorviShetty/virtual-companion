{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe869df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 11:35:25.812234: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-10 11:36:00.174785: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 11:36:00.177102: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 11:36:00.177193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1d7ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version                                               data\n",
       "0        1  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
       "1        1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
       "2        1  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
       "3        1  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
       "4        1  {'source': 'gutenberg', 'id': '3urfvvm165iantk..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
    "# coqa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6c16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del coqa[\"version\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcda5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required columns in our dataframe\n",
    "# cols = [\"text\",\"question\",\"answer\"]#list of lists to create our dataframe\n",
    "# comp_list = []\n",
    "# for index, row in coqa.iterrows():\n",
    "#     for i in range(len(row[\"data\"][\"questions\"])):\n",
    "#         temp_list = []\n",
    "#         temp_list.append(row[\"data\"][\"story\"])\n",
    "#         temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
    "#         temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
    "#         comp_list.append(temp_list)\n",
    "# new_df = pd.DataFrame(comp_list, columns=cols)\n",
    "# new_df.to_csv(\"../data/CoQA_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe304235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>When was the Vat formally opened?</td>\n",
       "      <td>It was formally established in 1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what is the library for?</td>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>for what subjects?</td>\n",
       "      <td>history, and law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>and?</td>\n",
       "      <td>philosophy, science and theology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what was started in 2014?</td>\n",
       "      <td>a  project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The Vatican Apostolic Library (), more commonl...   \n",
       "1  The Vatican Apostolic Library (), more commonl...   \n",
       "2  The Vatican Apostolic Library (), more commonl...   \n",
       "3  The Vatican Apostolic Library (), more commonl...   \n",
       "4  The Vatican Apostolic Library (), more commonl...   \n",
       "\n",
       "                            question                               answer  \n",
       "0  When was the Vat formally opened?  It was formally established in 1475  \n",
       "1           what is the library for?                             research  \n",
       "2                 for what subjects?                     history, and law  \n",
       "3                               and?     philosophy, science and theology  \n",
       "4          what was started in 2014?                           a  project  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/CoQA_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdaaf0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question and answers:  108647\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of question and answers: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8420937a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f330e2cab57d44ba9e9a78191cf2217d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1924ac5bc144b7acf3075e00dbb5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3829650f62a4a23b6593c5a2e35eaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d56eed26e8445398247f53f3e748ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "123fc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASKING QUESTIONS\n",
    "random_num = np.random.randint(0,len(data))\n",
    "question = data[\"question\"][random_num]\n",
    "text = data[\"text\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d667ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 376 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, text)\n",
    "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc92667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        101\n",
      "where      2,073\n",
      "is         2,003\n",
      "he         2,002\n",
      "being      2,108\n",
      "held       2,218\n",
      "?          1,029\n",
      "[SEP]        102\n",
      "chapter    3,127\n",
      "16         2,385\n",
      ":          1,024\n",
      "a          1,037\n",
      "treasure   8,813\n",
      "room       2,282\n",
      ".          1,012\n",
      "\"          1,000\n",
      "'          1,005\n",
      "tis       22,320\n",
      "infamous  14,429\n",
      ",          1,010\n",
      "\"          1,000\n",
      "ca         6,187\n",
      "##cam     28,727\n",
      "##a        2,050\n",
      "said       2,056\n",
      ",          1,010\n",
      "as         2,004\n",
      "he         2,002\n",
      "paced     13,823\n",
      "up         2,039\n",
      "and        1,998\n",
      "down       2,091\n",
      "the        1,996\n",
      "room       2,282\n",
      ";          1,025\n",
      "\"          1,000\n",
      "but        2,021\n",
      "what       2,054\n",
      "is         2,003\n",
      "to         2,000\n",
      "be         2,022\n",
      "done       2,589\n",
      "?          1,029\n",
      "they       2,027\n",
      "hold       2,907\n",
      "him        2,032\n",
      "in         1,999\n",
      "their      2,037\n",
      "hands      2,398\n",
      "as         2,004\n",
      "a          1,037\n",
      "hostage   13,446\n",
      ",          1,010\n",
      "in         1,999\n",
      "the        1,996\n",
      "heart      2,540\n",
      "of         1,997\n",
      "his        2,010\n",
      "own        2,219\n",
      "capital    3,007\n",
      ",          1,010\n",
      "and        1,998\n",
      "among      2,426\n",
      "his        2,010\n",
      "own        2,219\n",
      "people     2,111\n",
      ";          1,025\n",
      "and        1,998\n",
      "are        2,024\n",
      "capable    5,214\n",
      "of         1,997\n",
      "hanging    5,689\n",
      "him        2,032\n",
      "from       2,013\n",
      "the        1,996\n",
      "walls      3,681\n",
      ",          1,010\n",
      "should     2,323\n",
      "a          1,037\n",
      "hostile   10,420\n",
      "movement   2,929\n",
      "be         2,022\n",
      "made       2,081\n",
      "against    2,114\n",
      "them       2,068\n",
      ".          1,012\n",
      "\"          1,000\n",
      "you        2,017\n",
      "were       2,020\n",
      "right      2,157\n",
      ",          1,010\n",
      "roger      5,074\n",
      "hawks     12,505\n",
      "##haw     14,238\n",
      ",          1,010\n",
      "in         1,999\n",
      "warning    5,432\n",
      "us         2,149\n",
      "against    2,114\n",
      "these      2,122\n",
      "men        2,273\n",
      ".          1,012\n",
      "they       2,027\n",
      "are        2,024\n",
      "without    2,302\n",
      "faith      4,752\n",
      "and        1,998\n",
      "honor      3,932\n",
      ",          1,010\n",
      "thus       2,947\n",
      "to         2,000\n",
      "seize     15,126\n",
      "a          1,037\n",
      "host       3,677\n",
      "who        2,040\n",
      "has        2,038\n",
      "loaded     8,209\n",
      "them       2,068\n",
      "with       2,007\n",
      "presents   7,534\n",
      ",          1,010\n",
      "who        2,040\n",
      "has        2,038\n",
      "emptied   21,764\n",
      "his        2,010\n",
      "tre       29,461\n",
      "##as       3,022\n",
      "##uri      9,496\n",
      "##es       2,229\n",
      "to         2,000\n",
      "app       10,439\n",
      "##ease    19,500\n",
      "their      2,037\n",
      "greed     22,040\n",
      ",          1,010\n",
      "and        1,998\n",
      "who        2,040\n",
      "has        2,038\n",
      "treated    5,845\n",
      "them       2,068\n",
      "with       2,007\n",
      "the        1,996\n",
      "most       2,087\n",
      "extraordinary   9,313\n",
      "conde     24,707\n",
      "##sc      11,020\n",
      "##ens      6,132\n",
      "##ion      3,258\n",
      ".          1,012\n",
      "it         2,009\n",
      "is         2,003\n",
      "a          1,037\n",
      "crime      4,126\n",
      "un         4,895\n",
      "##heard   26,362\n",
      "of         1,997\n",
      ",          1,010\n",
      "an         2,019\n",
      "act        2,552\n",
      "of         1,997\n",
      "base       2,918\n",
      "ing       13,749\n",
      "##rat      8,609\n",
      "##itude   18,679\n",
      ",          1,010\n",
      "without    2,302\n",
      "a          1,037\n",
      "parallel   5,903\n",
      ".          1,012\n",
      "what       2,054\n",
      "is         2,003\n",
      "to         2,000\n",
      "be         2,022\n",
      "done       2,589\n",
      "?          1,029\n",
      "\"          1,000\n",
      "roger      5,074\n",
      "was        2,001\n",
      "silent     4,333\n",
      ".          1,012\n",
      "such       2,107\n",
      "a          1,037\n",
      "situation   3,663\n",
      ",          1,010\n",
      "so         2,061\n",
      "strange    4,326\n",
      "and        1,998\n",
      "un         4,895\n",
      "##lo       4,135\n",
      "##oked    23,461\n",
      "for        2,005\n",
      ",          1,010\n",
      "con        9,530\n",
      "##founded  21,001\n",
      "him        2,032\n",
      ".          1,012\n",
      "\"          1,000\n",
      "i          1,045\n",
      "should     2,323\n",
      "say        2,360\n",
      ",          1,010\n",
      "\"          1,000\n",
      "cu        12,731\n",
      "##it       4,183\n",
      "##cat     11,266\n",
      "##l        2,140\n",
      "burst      6,532\n",
      "out        2,041\n",
      "passionate  13,459\n",
      "##ly       2,135\n",
      ",          1,010\n",
      "\"          1,000\n",
      "that       2,008\n",
      "every      2,296\n",
      "mexican    4,916\n",
      "should     2,323\n",
      "take       2,202\n",
      "up         2,039\n",
      "arms       2,608\n",
      ",          1,010\n",
      "and        1,998\n",
      "ann        5,754\n",
      "##ih      19,190\n",
      "##ila     11,733\n",
      "##te       2,618\n",
      "this       2,023\n",
      "handful    9,210\n",
      "of         1,997\n",
      "invaders  17,347\n",
      ".          1,012\n",
      "what       2,054\n",
      "though     2,295\n",
      "monte     10,125\n",
      "##zu       9,759\n",
      "##ma       2,863\n",
      "fall       2,991\n",
      "?          1,029\n",
      "better     2,488\n",
      "that       2,008\n",
      "a          1,037\n",
      "monarch   11,590\n",
      "should     2,323\n",
      "per        2,566\n",
      "##ish      4,509\n",
      "than       2,084\n",
      "a          1,037\n",
      "nation     3,842\n",
      ".          1,012\n",
      "besides    4,661\n",
      ",          1,010\n",
      "monte     10,125\n",
      "##zu       9,759\n",
      "##ma       2,863\n",
      "has        2,038\n",
      "shown      3,491\n",
      "himself    2,370\n",
      "unfit     29,439\n",
      "to         2,000\n",
      "govern    21,208\n",
      ".          1,012\n",
      "it         2,009\n",
      "is         2,003\n",
      "his        2,010\n",
      "weakness  11,251\n",
      "that       2,008\n",
      "has        2,038\n",
      "brought    2,716\n",
      "things     2,477\n",
      "to         2,000\n",
      "this       2,023\n",
      "pass       3,413\n",
      ".          1,012\n",
      "think      2,228\n",
      "you        2,017\n",
      "that       2,008\n",
      "the        1,996\n",
      "white      2,317\n",
      "men        2,273\n",
      "could      2,071\n",
      "ever       2,412\n",
      "have       2,031\n",
      "advanced   3,935\n",
      "beyond     3,458\n",
      "the        1,996\n",
      "plateau    9,814\n",
      "of         1,997\n",
      "t          1,056\n",
      "##las      8,523\n",
      "##cala    25,015\n",
      ",          1,010\n",
      "had        2,018\n",
      "all        2,035\n",
      "the        1,996\n",
      "forces     2,749\n",
      "of         1,997\n",
      "mexico     3,290\n",
      "barred    15,605\n",
      "the        1,996\n",
      "way        2,126\n",
      "?          1,029\n",
      "think      2,228\n",
      "you        2,017\n",
      "that       2,008\n",
      "they       2,027\n",
      "could      2,071\n",
      "ever       2,412\n",
      "have       2,031\n",
      "entered    3,133\n",
      "the        1,996\n",
      "capital    3,007\n",
      ",          1,010\n",
      "had        2,018\n",
      "it         2,009\n",
      "been       2,042\n",
      "defended   8,047\n",
      "with       2,007\n",
      "resolution   5,813\n",
      "?          1,029\n",
      "one        2,028\n",
      "moment     2,617\n",
      "he         2,002\n",
      "flat       4,257\n",
      "##tered   14,050\n",
      "the        1,996\n",
      "strangers  12,358\n",
      "and        1,998\n",
      "loaded     8,209\n",
      "them       2,068\n",
      "with       2,007\n",
      "gifts      9,604\n",
      ";          1,025\n",
      "the        1,996\n",
      "next       2,279\n",
      "he         2,002\n",
      "was        2,001\n",
      "ready      3,201\n",
      "to         2,000\n",
      "send       4,604\n",
      "his        2,010\n",
      "forces     2,749\n",
      "against    2,114\n",
      "them       2,068\n",
      ".          1,012\n",
      "the        1,996\n",
      "cho       16,480\n",
      "##lu       7,630\n",
      "##lan      5,802\n",
      "##s        2,015\n",
      "had        2,018\n",
      "good       2,204\n",
      "reason     3,114\n",
      "for        2,005\n",
      "believing   8,929\n",
      "that       2,008\n",
      "he         2,002\n",
      "designed   2,881\n",
      "the        1,996\n",
      "ann        5,754\n",
      "##ih      19,190\n",
      "##ilation  29,545\n",
      "of         1,997\n",
      "the        1,996\n",
      "whites    12,461\n",
      ",          1,010\n",
      "if         2,065\n",
      "he         2,002\n",
      "did        2,106\n",
      "not        2,025\n",
      "actually   2,941\n",
      "order      2,344\n",
      "the        1,996\n",
      "attack     2,886\n",
      "upon       2,588\n",
      "them       2,068\n",
      ".          1,012\n",
      "[SEP]        102\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    print('{:8}{:8,}'.format(token,id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47715c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP token index:  7\n",
      "Number of tokens in segment A:  8\n",
      "Number of tokens in segment B:  368\n"
     ]
    }
   ],
   "source": [
    "#first occurence of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(\"SEP token index: \", sep_idx)#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
    "num_seg_a = sep_idx+1\n",
    "print(\"Number of tokens in segment A: \", num_seg_a)#number of tokens in segment B (text)\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(\"Number of tokens in segment B: \", num_seg_b)#creating the segment ids\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b#making sure that every input token has a segment id\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b97fa3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9ca171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "Where is he being held?\n",
      "\n",
      "Answer:\n",
      "In the heart of his own capital , and among his own people.\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bde0d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "    \n",
    "    #tokenize question and text as a pair\n",
    "    input_ids = tokenizer.encode(question, text)\n",
    "    \n",
    "    #string version of tokenized ids\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    #segment IDs\n",
    "    #first occurence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)    #number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1    #number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    #list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b    \n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    #model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "    \n",
    "    #reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)    \n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Please enter your text: \\n\")\n",
    "question = input(\"\\nPlease enter your question: \\n\")\n",
    "while True:\n",
    "    question_answer(question, text)\n",
    "    \n",
    "    flag = True\n",
    "    flag_N = False\n",
    "    \n",
    "    while flag:\n",
    "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
    "        if response[0] == \"Y\":\n",
    "            question = input(\"\\nPlease enter your question: \\n\")\n",
    "            flag = False\n",
    "        elif response[0] == \"N\":\n",
    "            print(\"\\nBye!\")\n",
    "            flag = False\n",
    "            flag_N = True\n",
    "            \n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6a558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
