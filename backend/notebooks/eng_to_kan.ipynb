{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0uptOB6U7GW",
        "outputId": "0f32105d-36a7-4233-c4c9-b8d5ab52595f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/testing\n"
          ]
        }
      ],
      "source": [
        "# create a seperate folder to store everything\n",
        "!mkdir testing\n",
        "%cd testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQFRiLtSalzt",
        "outputId": "7d405948-4365-4619-f5aa-b528053ff2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 697, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 697 (delta 278), reused 344 (delta 240), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (697/697), 2.64 MiB | 3.86 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n",
            "/content/testing/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1370, done.\u001b[K\n",
            "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 1370 (delta 114), reused 104 (delta 95), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1370/1370), 9.56 MiB | 13.19 MiB/s, done.\n",
            "Resolving deltas: 100% (724/724), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 39.64 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 3.11 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "/content/testing\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHUQGCACVvVf",
        "outputId": "4a2dd820-f012-4e5c-b9d4-2669765fd246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.2-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-1.2.2-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (3.5.4)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.16)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.27.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=94066b6c84d6d042eac79a8e9c6be8025fd5945ecfd10ddda8293e69e6076e38\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.0.2 morfessor-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1 sacremoses-0.0.53 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.2.2 sphinxcontrib-jquery-4.1 tensorboardX-2.6\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34735, done.\u001b[K\n",
            "remote: Counting objects: 100% (192/192), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 34735 (delta 98), reused 144 (delta 70), pack-reused 34543\u001b[K\n",
            "Receiving objects: 100% (34735/34735), 25.00 MiB | 26.15 MiB/s, done.\n",
            "Resolving deltas: 100% (25199/25199), done.\n",
            "/content/testing/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/testing/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.29.34)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.22.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2022.10.31)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.3.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.65.0)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.1)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->fairseq==0.12.2) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->fairseq==0.12.2) (16.0.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=22146173 sha256=0dbe88df4d9e70b86d7289d432ce394636f5f49fd1fc50d26b0162f19bea7546\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-10odh0on/wheels/b8/6b/1d/ce366a7474a3c4b61c48c8e95cfd7fd27822be96ca1beb2e59\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=e3e7591c19faeb6bf2c5968dd9b3294055db174c3c61a4113eaf23af9ff1642e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.5 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, xformers\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.29 typing-inspect-0.9.0 xformers-0.0.20\n",
            "/content/testing\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6SGeKxq1XPI"
      },
      "outputs": [],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKA8afhBawO5",
        "outputId": "9a84b8fc-706e-4fe2-fd12-17e863646ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-14 11:55:06--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 164.52.210.97, 164.52.206.155, 101.53.152.30, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|164.52.210.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4811880516 (4.5G) [application/zip]\n",
            "Saving to: ‘en2indic.zip’\n",
            "\n",
            "en2indic.zip        100%[===================>]   4.48G  11.5MB/s    in 6m 53s  \n",
            "\n",
            "2023-06-14 12:02:01 (11.1 MB/s) - ‘en2indic.zip’ saved [4811880516/4811880516]\n",
            "\n",
            "Archive:  en2indic.zip\n",
            "   creating: en-indic/\n",
            "   creating: en-indic/vocab/\n",
            "  inflating: en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: en-indic/vocab/vocab.SRC  \n",
            "  inflating: en-indic/vocab/vocab.TGT  \n",
            "  inflating: en-indic/vocab/bpe_codes.32k.TGT  \n",
            "   creating: en-indic/final_bin/\n",
            "  inflating: en-indic/final_bin/preprocess.log  \n",
            "  inflating: en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: en-indic/model/\n",
            "  inflating: en-indic/model/checkpoint_best.pt  \n",
            "/content/testing/indicTrans\n"
          ]
        }
      ],
      "source": [
        "# download the indictrans model\n",
        "\n",
        "\n",
        "# downloading the indic-en model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
        "# !unzip indic2en.zip\n",
        "\n",
        "# downloading the en-indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "!unzip en2indic.zip\n",
        "\n",
        "# # downloading the indic-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
        "# !unzip m2m.zip\n",
        "\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg1sQFfyWJli"
      },
      "outputs": [],
      "source": [
        "# creating a text file and adding en sentences we can use for testing the model\n",
        "!touch en_sentences.txt\n",
        "!echo 'This bicycle is too small for you !!' >> en_sentences.txt\n",
        "!echo \"I will directly meet you at the airport.\" >> en_sentences.txt\n",
        "!echo 'If COVID-19 is spreading in your community, stay safe by taking some simple precautions, such as physical distancing, wearing a mask, keeping rooms well ventilated, avoiding crowds, cleaning your hands, and coughing into a bent elbow or tissue' >> en_sentences.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a text file and adding en sentences we can use for testing the model\n",
        "!touch kn_google_outputs.txt\n",
        "!echo 'ಈ ಬೈಸಿಕಲ್ ನಿಮಗೆ ತುಂಬಾ ಚಿಕ್ಕದಾಗಿದೆ !!' >> kn_google_outputs.txt\n",
        "!echo \"ನಾನು ನಿಮ್ಮನ್ನು ನೇರವಾಗಿ ವಿಮಾನ ನಿಲ್ದಾಣದಲ್ಲಿ ಭೇಟಿಯಾಗುತ್ತೇನೆ.\" >> kn_google_outputs.txt\n",
        "!echo 'ನಿಮ್ಮ ಸಮುದಾಯದಲ್ಲಿ COVID-19 ಹರಡುತ್ತಿದ್ದರೆ, ದೈಹಿಕ ಅಂತರ, ಮಾಸ್ಕ್ ಧರಿಸುವುದು, ಕೊಠಡಿಗಳನ್ನು ಚೆನ್ನಾಗಿ ಗಾಳಿ ಇಟ್ಟುಕೊಳ್ಳುವುದು, ಜನಸಂದಣಿಯನ್ನು ತಪ್ಪಿಸುವುದು, ನಿಮ್ಮ ಕೈಗಳನ್ನು ಸ್ವಚ್ಛಗೊಳಿಸುವುದು ಮತ್ತು ಬಾಗಿದ ಮೊಣಕೈ ಅಥವಾ ಅಂಗಾಂಶಕ್ಕೆ ಕೆಮ್ಮುವುದು ಮುಂತಾದ ಕೆಲವು ಸರಳ ಮುನ್ನೆಚ್ಚರಿಕೆಗಳನ್ನು ತೆಗೆದುಕೊಳ್ಳುವ ಮೂಲಕ ಸುರಕ್ಷಿತವಾಗಿರಿ.' >> kn_google_outputs.txt"
      ],
      "metadata": {
        "id": "w9YSqK9LgzdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLg9BWAGWvLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d407311-261a-4680-e7d1-3f38ea107df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed 14 Jun 2023 12:03:14 PM UTC\n",
            "Applying normalization and script conversion\n",
            "100% 3/3 [00:00<00:00, 68.22it/s]\n",
            "Number of sentences in input: 3\n",
            "Applying BPE\n",
            "Decoding\n",
            "Extracting translations, script conversion and detokenization\n",
            "Translation completed\n"
          ]
        }
      ],
      "source": [
        "# joint_translate takes src_file, output_fname, src_lang, tgt_lang, model_folder as inputs\n",
        "# src_file -> input text file to be translated\n",
        "# output_fname -> name of the output file (will get created) containing the model predictions\n",
        "# src_lang -> source lang code of the input text ( in this case we are using en-indic model and hence src_lang would be 'en')\n",
        "# tgt_lang -> target lang code of the input text ( tgt lang for en-indic model would be any of the 11 indic langs we trained on:\n",
        "#              as, bn, hi, gu, kn, ml, mr, or, pa, ta, te)\n",
        "# supported languages are:\n",
        "#              as - assamese, bn - bengali, gu - gujarathi, hi - hindi, kn - kannada,\n",
        "#              ml - malayalam, mr - marathi, or - oriya, pa - punjabi, ta - tamil, te - telugu\n",
        "\n",
        "# model_dir -> the directory containing the model and the vocab files\n",
        "\n",
        "# Note: if the translation is taking a lot of time, please tune the buffer_size and batch_size parameter for fairseq-interactive defined inside this joint_translate script\n",
        "\n",
        "\n",
        "!bash joint_translate.sh en_sentences.txt kn_outputs.txt 'en' 'kn' '../en-indic'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xcnDOc4gNKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f01133a-353e-45d4-c932-861edbc4241b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 51.6,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
            " \"verbose_score\": \"81.6/60.9/44.2/35.0 (BP = 0.980 ratio = 0.980 hyp_len = 49 ref_len = 50)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.3.1\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! bash compute_bleu.sh kn_outputs.txt kn_google_outputs.txt en kn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QzkBCgeGZiH",
        "outputId": "3c25a1cf-f0dc-4904-a213-316043661ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ಈ ಸೈಕಲ್ ನಿಮಗೆ ತುಂಬಾ ಚಿಕ್ಕದಾಗಿದೆ!\n",
            "ನಾನು ನಿಮ್ಮನ್ನು ನೇರವಾಗಿ ವಿಮಾನ ನಿಲ್ದಾಣದಲ್ಲಿ ಭೇಟಿ ಮಾಡುತ್ತೇನೆ.\n",
            "ನಿಮ್ಮ ಸಮುದಾಯದಲ್ಲಿ ಕೋವಿಡ್-19 ಹರಡುತ್ತಿದ್ದರೆ, ದೈಹಿಕ ಅಂತರ, ಮುಖಗವಸು ಧರಿಸುವುದು, ಕೋಣೆಗಳಲ್ಲಿ ಗಾಳಿಯನ್ನು ಚೆನ್ನಾಗಿ ಇಟ್ಟುಕೊಳ್ಳುವುದು, ಜನಸಮೂಹವನ್ನು ತಪ್ಪಿಸುವುದು, ನಿಮ್ಮ ಕೈಗಳನ್ನು ಸ್ವಚ್ಛಗೊಳಿಸುವುದು ಮತ್ತು ಮೊಣಕೈ ಅಥವಾ ಅಂಗಾಂಶದಲ್ಲಿ ಕೆಮ್ಮುವುದು ಮುಂತಾದ ಕೆಲವು ಸರಳ ಮುನ್ನೆಚ್ಚರಿಕೆಗಳನ್ನು ತೆಗೆದುಕೊಳ್ಳುವ ಮೂಲಕ ಸುರಕ್ಷಿತವಾಗಿರಿ.\n"
          ]
        }
      ],
      "source": [
        "!cat kn_outputs.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YK2BdwvrUgI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "interpreter": {
      "hash": "3c7d4130300118f0c7487d576c6841c0dbbdeec039e1e658ac9b107412a09af0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}